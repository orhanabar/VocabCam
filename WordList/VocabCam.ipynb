{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ed4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cce2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import string \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "import string \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from PyDictionary import PyDictionary\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "code_list1 = list()\n",
    "def vocab_cam(file_name1,trans_lang,stem_check):\n",
    "    def File_Content(file_name):\n",
    "        file_str = ''\n",
    "\n",
    "        if file_name.endswith('.pdf'):\n",
    "            pdf_file = convert_from_path(file_name)\n",
    "\n",
    "            for i in range(len(pdf_file)):\n",
    "                pdf_file[i].save(f'page{str(i + 1)}.jpg', 'JPEG')   # Saves pages as images in the pdf\n",
    "                file = f'page{str(i + 1)}.jpg'\n",
    "                file_str += pytesseract.image_to_string(file)\n",
    "                # os.remove(file)   # Deletes saved image files\n",
    "        elif file_name.endswith(('.jpg', '.png')):\n",
    "            file_str = pytesseract.image_to_string(file_name)   \n",
    "        return file_str\n",
    "    file_cont= File_Content(file_name1)\n",
    "    \n",
    "    with open(\"Deneme.txt\",\"a\",encoding=\"utf-8\") as file:\n",
    "        file.write(file_cont)  \n",
    "    with open(\"Deneme.txt\",\"r+\",encoding=\"utf-8\") as file:\n",
    "        content= file.readlines()\n",
    "        parag=  file.read()\n",
    "        untitled=\"\"\n",
    "        for i in content:\n",
    "            if(i.isupper()==True or (i.istitle()==True)):\n",
    "                untitled += \" \"\n",
    "            else:\n",
    "                untitled += i + \" \"\n",
    "    with open(\"Computer Reading Son.txt\",\"a\",encoding=\"utf-8\") as file1:\n",
    "        for i in untitled:\n",
    "            file1.write(i)\n",
    "    with open(\"Computer Reading Son.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "        contents=file.read()\n",
    "        content_clear =contents.replace(\"\\n\",\"\")\n",
    "        content_clear = content_clear.replace(\"/n\",\"\")\n",
    "        def punctuation_clear(text):\n",
    "            result = \"\"\n",
    "            for i in text:\n",
    "                if( i not in string.punctuation and i.isalpha() != False or i == \" \"  ):\n",
    "                    result+= i \n",
    "                else:\n",
    "                    result+= \"\"\n",
    "            return result                \n",
    "        \n",
    "        def punctuation_clear_list(text):\n",
    "            result = \"\"\n",
    "            text=text.lower()\n",
    "            for i in text:\n",
    "                for a in i:\n",
    "                    if( a not in string.punctuation and a.isalpha() or a == \".\" or a == \" \" ):\n",
    "                        result+= a \n",
    "            return result\n",
    "        def stopword(x):\n",
    "            result = \"\"\n",
    "            text_tokens = word_tokenize(x)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens_without_sw= [word for word in text_tokens if not word.lower() in stop_words]\n",
    "            return tokens_without_sw\n",
    "    \n",
    "        content_clears = punctuation_clear(content_clear)\n",
    "        content_split_sentence = punctuation_clear_list(content_clear).split(\".\")\n",
    "        content_word_split= set(content_clears.split())\n",
    "        word_list= \"\"\n",
    "        word_list1 = \"\"\n",
    "\n",
    "        for i in content_word_split:\n",
    "            if(len(i)>=3):\n",
    "                i = i.lower()\n",
    "                i=punctuation_clear(i)\n",
    "                if(len(word_list)==0):\n",
    "                    word_list+=i\n",
    "                else:\n",
    "                    word_list+=\" \"+i\n",
    "        word_list= stopword(word_list)\n",
    "        word_list=set(word_list)\n",
    "        word_list=list(word_list)\n",
    "        \n",
    "        say1=0\n",
    "        for q in word_list:\n",
    "            say1+=1\n",
    "            if(len(word_list1)==0):\n",
    "                word_list1+=q\n",
    "            else:\n",
    "                word_list1+=\",\"+q\n",
    "        print(\"Kelime Listesi : \",say1)\n",
    "    \n",
    "    ## CÜMLE AYIRMA KISMI\n",
    "    sentence_list_last = list()\n",
    "    for w in word_list:\n",
    "        \n",
    "        num=0\n",
    "        for r in content_split_sentence:\n",
    "            sentence = r.split()\n",
    "            for e in sentence:\n",
    "                if w==e:\n",
    "                    sentence_list_last.append(r)\n",
    "                    num+=1\n",
    "                if num==1:\n",
    "                    break\n",
    "            if num==1:\n",
    "                break\n",
    "    say2=0\n",
    "    for q in sentence_list_last:\n",
    "        say2+=1\n",
    "    print(\"Cümle Listesi : \",say2)            \n",
    "    content_clear_split_last = list()\n",
    "    \n",
    "    ##KÖK KISMI\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stem_list=[]\n",
    "    if(stem_check==True):\n",
    "        for i in word_list:\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            stem_list.append(stemmer.stem(i))\n",
    "        say3=0\n",
    "        for q in stem_list:\n",
    "            say3+=1\n",
    "        print(\"kök Listesi : \",say3)\n",
    "    \n",
    "   ##ÇEVİRİ KISMI  \n",
    "    from googletrans import Translator\n",
    "    translator = Translator()\n",
    "    trans_list= str(word_list)\n",
    "    trans_list = trans_list.strip(\"[]\")    \n",
    "    translation = translator.translate(trans_list,dest = trans_lang)\n",
    "    trans_list= translation.text\n",
    "    trans_list=trans_list.replace(\"'\",\"\")\n",
    "    trans_list=trans_list.split(\",\")\n",
    "\n",
    "    \n",
    "    say4=0\n",
    "    for q in trans_list:\n",
    "        say4+=1\n",
    "    print(\"çeviri Listesi : \",say4)\n",
    "    \n",
    "    \n",
    "    ## ETİKETLEME KISMI \n",
    "    from nltk.corpus import state_union\n",
    "    import nltk\n",
    "    from nltk.tokenize import PunktSentenceTokenizer\n",
    "    code_list = list()\n",
    "    for i in word_list:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        code_list.append(tagged[0][1])\n",
    "    say5=0\n",
    "    for q in code_list:\n",
    "        say5+=1\n",
    "        code_list1.append(q)\n",
    "    print(\"kod Listesi : \",say5)\n",
    "    dictionary = PyDictionary()\n",
    "    pydictlist = list()\n",
    "    counter = 0\n",
    "    for i in code_list:\n",
    "        try:\n",
    "            if(i==\"NN\" or i==\"NNS\" or i==\"NNP\" or i == \"NNPS\"):\n",
    "                definition=dictionary.meaning(word_list[counter],disable_errors=True)[\"Noun\"][0]\n",
    "                pydictlist.append(definition)\n",
    "            elif(i == \"VB\" or i == \"VBD\" or i == \"VBG\" or i == \"VBN\" or i == \"VBP\" or i  == \"VBZ\" ):\n",
    "                definition=dictionary.meaning(word_list[counter],disable_errors=True)[\"Verb\"][0]\n",
    "                pydictlist.append(definition)\n",
    "            else:\n",
    "                pydictlist.append(\"None\")\n",
    "        except TypeError:\n",
    "            pydictlist.append(\"None\")\n",
    "        except KeyError:\n",
    "            pydictlist.append(\"None\")\n",
    "        except ValueError:\n",
    "            pydictlist.append(\"None\")\n",
    "        except IndexError:\n",
    "            pydictlist.append(\"None\")\n",
    "        counter +=1\n",
    "    if(stem_check==True):\n",
    "        dict = {\"Code:\":code_list,\"Raw\":word_list,\"Root\":stem_list,\"Translation\":trans_list,\"Eng Translation\":pydictlist,\"Sentence\":sentence_list_last,}\n",
    "        df= pd.DataFrame(dict)\n",
    "        df.to_json(\"Computer Reading Translate.json\")\n",
    "    else:\n",
    "        dict = {\"Code:\":code_list,\"Raw\":word_list,\"Translation\":ceviri_list,\"Eng Translation\":pydictlist,\"Sentence\":sentence_list_last,}\n",
    "        df= pd.DataFrame(dict)\n",
    "        df.to_json(\"Computer Reading Translate.json\")\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06023b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Listesi :  354\n",
      "Cümle Listesi :  354\n",
      "kök Listesi :  354\n",
      "çeviri Listesi :  354\n",
      "kod Listesi :  354\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code:</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Root</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Eng Translation</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>built</td>\n",
       "      <td>built</td>\n",
       "      <td>inşa</td>\n",
       "      <td>None</td>\n",
       "      <td>this enables computers to be built into other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JJ</td>\n",
       "      <td>advanced</td>\n",
       "      <td>advanc</td>\n",
       "      <td>gelişmiş</td>\n",
       "      <td>None</td>\n",
       "      <td>advanced systems known as expert systems enab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN</td>\n",
       "      <td>plastic</td>\n",
       "      <td>plastic</td>\n",
       "      <td>plastik</td>\n",
       "      <td>generic name for certain synthetic or semisynt...</td>\n",
       "      <td>this enables computers to be built into other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN</td>\n",
       "      <td>decide</td>\n",
       "      <td>decid</td>\n",
       "      <td>karar</td>\n",
       "      <td>None</td>\n",
       "      <td>medical expert systems for example can help d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>process</td>\n",
       "      <td>process</td>\n",
       "      <td>süreç</td>\n",
       "      <td>a particular course of action intended to achi...</td>\n",
       "      <td>a multimedia computer can process different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>NN</td>\n",
       "      <td>webpage</td>\n",
       "      <td>webpag</td>\n",
       "      <td>web sayfası</td>\n",
       "      <td>a document connected to the World Wide Web and...</td>\n",
       "      <td>video ﬁles attached to simple email text mess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>JJ</td>\n",
       "      <td>threedimensional</td>\n",
       "      <td>threedimension</td>\n",
       "      <td>üç boyutlu</td>\n",
       "      <td>None</td>\n",
       "      <td>computer uses mentioned in this unit include ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>NN</td>\n",
       "      <td>server</td>\n",
       "      <td>server</td>\n",
       "      <td>sunucu</td>\n",
       "      <td>a person whose occupation is to serve at table...</td>\n",
       "      <td>an internet service called ftp file transfer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>JJ</td>\n",
       "      <td>possible</td>\n",
       "      <td>possibl</td>\n",
       "      <td>mümkün</td>\n",
       "      <td>None</td>\n",
       "      <td>it is also possible to build all the main ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>NN</td>\n",
       "      <td>style</td>\n",
       "      <td>style</td>\n",
       "      <td>stil</td>\n",
       "      <td>how something is done or how it happens</td>\n",
       "      <td>the main device for inputting the data is a t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Code:               Raw            Root   Translation  \\\n",
       "0      NN             built           built          inşa   \n",
       "1      JJ          advanced          advanc      gelişmiş   \n",
       "2      NN           plastic         plastic       plastik   \n",
       "3      NN            decide           decid         karar   \n",
       "4      NN           process         process         süreç   \n",
       "..    ...               ...             ...           ...   \n",
       "349    NN           webpage          webpag   web sayfası   \n",
       "350    JJ  threedimensional  threedimension    üç boyutlu   \n",
       "351    NN            server          server        sunucu   \n",
       "352    JJ          possible         possibl        mümkün   \n",
       "353    NN             style           style          stil   \n",
       "\n",
       "                                       Eng Translation  \\\n",
       "0                                                 None   \n",
       "1                                                 None   \n",
       "2    generic name for certain synthetic or semisynt...   \n",
       "3                                                 None   \n",
       "4    a particular course of action intended to achi...   \n",
       "..                                                 ...   \n",
       "349  a document connected to the World Wide Web and...   \n",
       "350                                               None   \n",
       "351  a person whose occupation is to serve at table...   \n",
       "352                                               None   \n",
       "353            how something is done or how it happens   \n",
       "\n",
       "                                              Sentence  \n",
       "0     this enables computers to be built into other...  \n",
       "1     advanced systems known as expert systems enab...  \n",
       "2     this enables computers to be built into other...  \n",
       "3     medical expert systems for example can help d...  \n",
       "4     a multimedia computer can process different f...  \n",
       "..                                                 ...  \n",
       "349   video ﬁles attached to simple email text mess...  \n",
       "350   computer uses mentioned in this unit include ...  \n",
       "351   an internet service called ftp file transfer ...  \n",
       "352      it is also possible to build all the main ...  \n",
       "353   the main device for inputting the data is a t...  \n",
       "\n",
       "[354 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_cam('Computer Reading.pdf',\"tr\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f8b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8dac34539f472889cfe55f2ed6694af900a23bf83e4a45908353ce84e7dbf49d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
