{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea5276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import string \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from googletrans import Translator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81af9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def File_Content(file_name):\n",
    "    file_str = ''\n",
    "\n",
    "    if file_name.endswith('.pdf'):\n",
    "        pdf_file = convert_from_path(file_name)\n",
    "\n",
    "        for i in range(len(pdf_file)):\n",
    "            pdf_file[i].save(f'page{str(i + 1)}.jpg', 'JPEG')   # Saves pages as images in the pdf\n",
    "            file = f'page{str(i + 1)}.jpg'\n",
    "            file_str += pytesseract.image_to_string(file)\n",
    "            # os.remove(file)   # Deletes saved image files\n",
    "    elif file_name.endswith(('.jpg', '.png')):\n",
    "        file_str = pytesseract.image_to_string(file_name)\n",
    "        \n",
    "    return file_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b90ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cont= File_Content('Computer Reading.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50414e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"Deneme.txt\",\"a\",encoding=\"utf-8\") as file:\n",
    "    file.write(file_cont)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa863202",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(\"Deneme.txt\",\"r+\",encoding=\"utf-8\") as file:\n",
    "        content= file.readlines()\n",
    "        parag=  file.read()\n",
    "    \n",
    "\n",
    "        untitled=\"\"\n",
    "        for i in content:\n",
    "            if(i.isupper()==True or (i.istitle()==True)):\n",
    "                untitled += \" \"\n",
    "            else:\n",
    "                untitled += i + \" \"\n",
    "    with open(\"Computer Reading Son.txt\",\"a\",encoding=\"utf-8\") as file1:\n",
    "        for i in untitled:\n",
    "            file1.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7be3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.tokenize import sent_tokenize\n",
    "with open(\"Computer Reading Son.txt\",\"r\",encoding=\"utf-8\") as file: \n",
    "    contents=file.read()\n",
    "    content_clear =contents.replace(\"\\n\",\"\")\n",
    "    content_clear = content_clear.replace(\"/n\",\"\")\n",
    "    \n",
    "    def punctuation_clear(text):\n",
    "        result = \"\"\n",
    "        for i in text:\n",
    "            if( i not in string.punctuation or i == \" \" ):\n",
    "                result+= i \n",
    "            else:\n",
    "                result+= \"\"\n",
    "        return result\n",
    "    def punctuation_clear_list(text):\n",
    "        result = \"\"\n",
    "        for i in text:\n",
    "            for a in i:\n",
    "                if( a not in string.punctuation or a == \".\" or a == \" \" ):\n",
    "                    result+= a \n",
    "            \n",
    "        return result\n",
    "    content_clears = punctuation_clear(content_clear)\n",
    "    content_split_sentence = punctuation_clear_list(content_clear).split(\".\")\n",
    "    content_word_split= set(content_clears.split())\n",
    "    word_list= list()\n",
    "    say = 0\n",
    "    for i in content_word_split:\n",
    "        if(len(i)>=3):\n",
    "            word_list.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fa4385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n"
     ]
    }
   ],
   "source": [
    "sentence_list_last = list()\n",
    "for a in word_list:\n",
    "    x=0\n",
    "    for i in content_split_sentence:\n",
    "        sentence = i.split()\n",
    "        for j in sentence:\n",
    "            if a==j:\n",
    "                sentence_list_last.append(i)\n",
    "                x+=1\n",
    "            if x==1:\n",
    "                break\n",
    "        if x==1:\n",
    "            break\n",
    "            \n",
    "content_clear_split_last = list()\n",
    "\n",
    "    \n",
    "say = 0\n",
    "for i in sentence_list_last:\n",
    "    say+=1\n",
    "print(say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5888add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stem_list=[]\n",
    "\n",
    "for i in word_list:\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stem_list.append(stemmer.stem(i))\n",
    "#stem_set = set(stem_list)\n",
    "#stem_list = list(stem_set)\n",
    "a=  0\n",
    "for i in stem_list:\n",
    "    a+=1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b875de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "ceviri_list= list()\n",
    "for i in word_list:\n",
    "    translation = translator.translate(i,dest = 'tr')\n",
    "    ceviri_list.append(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cacdcf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "code_list = list()\n",
    "for i in word_list:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        code_list.append(tagged[0][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55c930d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dict = {\"Code:\":code_list,\"Raw\":word_list,\"Root\":stem_list,\"Translation\":ceviri_list,\"Sentence\":sentence_list_last}\n",
    "df= pd.DataFrame(dict)\n",
    "df.to_csv(\"Computer Reading Translate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d171f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ed4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8dac34539f472889cfe55f2ed6694af900a23bf83e4a45908353ce84e7dbf49d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
